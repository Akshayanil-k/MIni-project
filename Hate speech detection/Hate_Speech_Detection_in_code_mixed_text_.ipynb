{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshayanil-k/MIni-project/blob/main/Hate%20speech%20detection/Hate_Speech_Detection_in_code_mixed_text_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhR9vGKAxlYU"
      },
      "source": [
        "importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw2MiPLExc8G",
        "outputId": "9617e8a9-1010-4833-de30-0bdbb3541206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_PsL8UPy8w8"
      },
      "source": [
        "Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kJ-fdshy8i8",
        "outputId": "aa11caaa-12d4-4d8e-97f4-16ac43526153"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0        3002\n",
              "1        2244\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset=pd.read_csv('/content/hate_speech_detection_1.csv')\n",
        "tweets=dataset.iloc[:,2].values\n",
        "test=[dataset.iloc[1,2]] #for checking the working of clean \n",
        "y=dataset.drop(['tweet','id'],axis='columns')\n",
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTtHSq-UzaKz"
      },
      "source": [
        "Cleaning text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqiPHn1czLBX",
        "outputId": "13d81769-44ab-4235-8870-2fd12034b967"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def clean(text):\n",
        "  STOPWORDS.update({'is','the','of','to','on','so'})\n",
        "  clean_tweet=[]\n",
        "  for i in range(len(text)):\n",
        "    sample=re.sub(r'[^a-zA-Z\\n]',\" \",text[i])\n",
        "    sample=re.sub(r\"[.,'\\\"@#!?]\",\" \",sample)\n",
        "    sample=re.sub('user',\" \",sample)\n",
        "    sample=sample.lower()\n",
        "    sample=sample.split()\n",
        "    lm=WordNetLemmatizer()\n",
        "    for word in sample:\n",
        "      word=lm.lemmatize(word)\n",
        "    sample=list(set(sample)-STOPWORDS)\n",
        "    sample=\" \".join(sample)\n",
        "    clean_tweet.append(sample) \n",
        "  return clean_tweet\n",
        "type(clean(tweets))#Showing the 10 clean tweets from dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nQeE6ywVYbCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfd78b6-d967-44e5-a263-74fd0e9fbe54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4VdLZpLkaDxN"
      },
      "outputs": [],
      "source": [
        "def get_predicton(tweet):\n",
        "  lis=[[]]\n",
        "  lis[0].append(tweet)\n",
        "  tweet=lis\n",
        "  tweet=clean(tweet[0])\n",
        "  lol=cv.transform(clean(tweet))\n",
        "  pred=rf.predict(lol)\n",
        "  if pred[0]==1:\n",
        "    result='Toxic tweet'\n",
        "  else:\n",
        "    result='Non-Toxic tweet'\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6AdniWb__UU"
      },
      "source": [
        "Creating tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2nCWNr5Y__Ar"
      },
      "outputs": [],
      "source": [
        "cv=CountVectorizer(max_features=5000)\n",
        "x=cv.fit_transform(clean(tweets)).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES7fsPpMT0hT"
      },
      "source": [
        "Tackling imbalence in dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "42C6uUbkTznv"
      },
      "outputs": [],
      "source": [
        "sm=SMOTE()\n",
        "x_sm,y_sm=sm.fit_resample(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9OGOsf5BUUkZ"
      },
      "outputs": [],
      "source": [
        "y_sm=y_sm['label'].values #making it nd array to feed the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVnzQvHiBC8n"
      },
      "source": [
        "Splitting dataset in to test and training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvFP38N72l2V",
        "outputId": "e8ae294b-9510-4269-e7ee-c436ea0bcb8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(x_sm,y_sm,test_size=0.3,random_state=0)\n",
        "type(ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkFUAEuhBp58"
      },
      "source": [
        "Training model on training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vbLpfIbVBpki"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf=RandomForestClassifier(n_estimators=50,criterion='gini',max_features='log2') #best parameter from Grid search is used in this model\n",
        "rf.fit(xtrain,ytrain)\n",
        "ypred=rf.predict(xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jDrKAPsrv_Q"
      },
      "source": [
        "Grid Search with cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYh7fFjl2sQ8",
        "outputId": "393d30ea-9f3d-41be-dbc6-7ccf1d9d2728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "30 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\", line 467, in fit\n",
            "    for i, t in enumerate(trees)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 1043, in __call__\n",
            "    if self.dispatch_one_batch(iterator):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 779, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
            "    result = ImmediateResult(func)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
            "    self.results = batch()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in __call__\n",
            "    for func, args, kwargs in self.items]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in <listcomp>\n",
            "    for func, args, kwargs in self.items]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
            "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 942, in fit\n",
            "    X_idx_sorted=X_idx_sorted,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 282, in fit\n",
            "    \"Invalid value for max_features. \"\n",
            "ValueError: Invalid value for max_features. Allowed string values are 'auto', 'sqrt' or 'log2'.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.82746419 0.82960365 0.82936498 0.84174509 0.84412321 0.84269492\n",
            "        nan        nan        nan 0.83031652 0.82889049 0.82413\n",
            " 0.84650161 0.84840581 0.84650218        nan        nan        nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Accuracy: 84.84 %\n",
            "best param: {'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 100}\n"
          ]
        }
      ],
      "source": [
        "param=[{\n",
        "    'n_estimators':[50,100,120],\n",
        "    'criterion':['gini','entropy'],\n",
        "    'max_features':['sqrt','log2','']}]\n",
        "gw=GridSearchCV(estimator=rf,param_grid=param,scoring='accuracy',cv=5)\n",
        "gw.fit(xtrain,ytrain)\n",
        "best_acc=gw.best_score_\n",
        "beat_param=gw.best_params_\n",
        "print('Best Accuracy: {:.2f} %'.format(best_acc*100) )\n",
        "print('best param:',beat_param )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUWkxQYoCiN1"
      },
      "source": [
        "Checking accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYFwDVBjByqz",
        "outputId": "87bd9ae8-f086-4200-a5cb-1bfd30deb17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report of the Random Forest model: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84       906\n",
            "           1       0.82      0.89      0.86       896\n",
            "\n",
            "    accuracy                           0.85      1802\n",
            "   macro avg       0.85      0.85      0.85      1802\n",
            "weighted avg       0.85      0.85      0.85      1802\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print('Classification Report of the Random Forest model: \\n',classification_report(ytest,ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrcZcvdLfG3h"
      },
      "source": [
        "Testing model on a random tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R4Yp97MrfGnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe40c9b9-c645-4624-cc79-60b6302f9ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-Toxic tweet\n"
          ]
        }
      ],
      "source": [
        "print(get_predicton(\"china phone elarum eduth upp velathil mukki iduka\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "760ivB8oVRug"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Hate Speech Detection in code mixed text .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}